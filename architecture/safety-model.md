# Safety Model – LawAI

## Purpose

This document describes the safety and responsibility mechanisms built into LawAI.  
Legal information is a high-risk domain, and incorrect or hallucinated responses can cause real-world harm.  
LawAI is explicitly designed to minimize these risks.

---

## Core Safety Principles

LawAI is built on the following non-negotiable safety principles:

1. **Legal Awareness Only**  
   The system provides educational information about laws and rights.  
   It does not provide legal advice, strategy, or outcome prediction.

2. **Authoritative Grounding**  
   All legal information is grounded in curated data derived from official sources such as India Code.  
   The AI itself is never treated as a source of truth.

3. **Schema-Governed Knowledge**  
   Every legal concept must conform to a master JSON schema before being ingested.  
   Invalid or incomplete data is rejected and never exposed to the AI.

---

## Hallucination Prevention

LawAI prevents hallucinations using multiple layers of control:

- The AI only receives retrieved legal concept data, not raw law text or external sources.
- The system prompt explicitly forbids the AI from using prior or external knowledge.
- If required information is not present in the retrieved concept, the AI must refuse to answer.

This ensures the AI never invents laws, sections, penalties, or procedures.

---

## Refusal and Safe Failure

LawAI enforces refusal in the following scenarios:

- The user asks for legal advice or outcome prediction
- The question falls outside available legal concepts
- The required legal information is missing from verified data
- The user requests actions that may cause harm or misuse

In all refusal cases, the system responds politely and transparently without guessing.

---

## Mandatory Disclaimers

Every response generated by LawAI includes a mandatory disclaimer stating that:
- The information is for legal awareness only
- It does not replace consultation with a qualified legal professional

Disclaimers are enforced programmatically and cannot be removed by the AI.

---

## Data Integrity and Updates

- Legal concepts are versioned and periodically reviewed.
- Updates are made by modifying the structured data, not by retraining the AI.
- Schema validation ensures backward compatibility and prevents breaking changes.

This approach ensures long-term maintainability and correctness.

---

## Why This Model Is Necessary

General-purpose AI systems are unsafe for legal use because they:
- May hallucinate laws or sections
- Lack source traceability
- Cannot enforce refusal reliably

LawAI’s safety model addresses these risks through controlled architecture rather than model-level assumptions.

---

## Summary

LawAI’s safety model ensures that:
- AI explanations are grounded and verifiable
- Incorrect or harmful responses are prevented
- Users receive trustworthy legal awareness without false confidence

This makes LawAI suitable for responsible, real-world deployment.
